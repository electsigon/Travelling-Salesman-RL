# -*- coding: utf-8 -*-
"""Structure2Vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11YoGuhj4JFR3HTfKPYmtrvHUBHPj4ZHC
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.autograd import Variable

from scipy.spatial.distance import pdist, squareform
import numpy as np
import random
import math
import time

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors

import tsp_with_ortools

use_cuda = False
gamma = 0.1 #Now the Q Value is going to be the length of the best tour
num_episodes = 2000
EPS_START = 0.8  # e-greedy threshold start value
EPS_END = 0.01  # e-greedy threshold end value
EPS_DECAY = 200  # e-greedy threshold decay
LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor
FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor
BATCH_SIZE = 64
lr=0.0001

def compute_tour_cost(tour, d_mat):
    cum_dist = 0
    for i in range(len(tour) - 1):
        me = tour[i]
        next = tour[i+1]
        d = d_mat[me, next]
        cum_dist += d
    cum_dist += d_mat[tour[-1], tour[0]]
    return cum_dist

class ReplayMemory:
    def __init__(self, capacity):
        self.capacity = capacity
        self.memory = []

    def push(self, transition):
        self.memory.append(transition)
        if len(self.memory) > self.capacity:
            del self.memory[0]

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

memory = ReplayMemory(50000)

class TSPEnv(object):
  def __init__(self):
    self.seq = np.array([])

  def reset(self, pre_train = False):
    seq = np.random.randint(100, size=(20, 2))
    pca = PCA(n_components = 2)
    sequence = pca.fit_transform(seq)
    self.seq = sequence/100
    self.current_vertices = np.zeros((20, 2), dtype=np.double)
    distance_matrix = squareform(pdist(self.seq))
    #neighbors = NearestNeighbors(n_neighbors = 10, metric = 'precomputed')
    #neighbors.fit(distance_matrix)
    #all_neighbors = neighbors.kneighbors(distance_matrix)[1]
    #self.distance_matrix = 10 * np.ones(distance_matrix.shape)
    #for v in range(20):
    #  vertex_neighbors = all_neighbors[v]
    #  for u in vertex_neighbors:
    #    self.distance_matrix[u, v] = distance_matrix[u, v]
    self.distance_matrix = distance_matrix
    self.real_distances = distance_matrix
    if(pre_train):
      solver = tsp_with_ortools.Solver(20)
      route, distance = solver.run(self.real_distances)
      route_vertices = np.zeros(20, dtype = np.double)
      route_vertices[0] = 1
      next_route_vertices = route_vertices
      current_vertex = 0
      cum_reward = 0
      for i in range(len(route) - 1):
        next_route_vertices[route[i+1]] = 1
        action = route[i+1]
        done = 0
        reward = self.real_distances[current_vertex, action]
        if(i == len(route) - 1):
          reward += self.real_distances[action, 0]
          done = 1
        cum_reward += reward
        memory.push((FloatTensor([route_vertices]),
                     FloatTensor([self.distance_matrix]),
                     LongTensor([[action]]),
                     FloatTensor([next_route_vertices]),
                     FloatTensor([self.distance_matrix]),
                     FloatTensor([-cum_reward]),
                     FloatTensor([done])))
        current_vertex = action
        route_vertices = next_route_vertices
      print("Actual Cumulative Reward: {0}".format(cum_reward))
    self.tour = [0]
    self.current_vertices[0][0] = 1
    self.current_vertices[0][1] = 1
    self.cum_distance = 0
    return self.current_vertices, self.distance_matrix

  def reset_tour(self):
    self.tour = [0]
    self.current_vertices = np.zeros((20, 2), dtype=np.double)
    self.current_vertices[0] = 1
    self.cum_distance = 0
    return self.current_vertices, self.distance_matrix

  def step(self, action):
    distance = self.real_distances[self.tour[-1], action]
    distance -= self.real_distances[self.tour[-1], 0]
    distance += self.real_distances[action, 0]
    self.cum_distance += distance
    if(self.current_vertices[action][0] == 1):
      return self.current_vertices, self.distance_matrix, -99, True, self.tour
    self.current_vertices[action][0] = 1
    self.current_vertices[self.tour[-1]][1] = 0
    self.current_vertices[action][1] = 1
    self.tour.append(action)
    done = len(self.tour) == 20
    if(done):
      print("My Tour: {0}".format(self.tour))
      print("My Total Distance: {0}".format(self.cum_distance))
    return self.current_vertices, self.distance_matrix, -distance, done, self.tour

  def close(self):
    return

class TestEnv(object):
  def __init__(self, folder):
    self.seq = np.array([])
    self.folder = folder
    self.file_names = []
    with open('%s/paths.txt' % self.folder, 'r') as f:
      for line in f:
        fname = "%s/%s" % (self.folder, line.split('/')[-1].strip())
        self.file_names.append(fname)
    self.file_index = 0

  def reset(self):
    fname = self.file_names[self.file_index]
    coors = []
    n_nodes = -1
    in_sec = False
    with open(fname, 'r') as f_tsp:
      for l in f_tsp:
        if 'DIMENSION' in l:
          n_nodes = int(l.split(' ')[-1].strip())
        if in_sec:
          id, x, y = [int(w.strip()) for w in l.split(' ')]
          coors.append([float(x) / 1000000.0, float(y) / 1000000.0])
        elif 'NODE_COORD_SECTION' in l:
          in_sec = True
    if(n_nodes < 20):
      for i in range(20 - n_nodes):
        coors.append(coors[0])
    or_sequence = 100.0 * np.array(coors)
    self.distance_matrix = squareform(pdist(or_sequence))
    self.current_vertices = np.zeros((20, 2), dtype=np.double)
    self.tour = [0]
    self.current_vertices[0] = 1
    self.file_index += 1
    return self.current_vertices, -self.distance_matrix

  def step(self, action):
    distance = self.distance_matrix[self.tour[-1], action]
    self.tour.append(action)
    if(self.current_vertices[action] == 1):
      return self.current_vertices, self.distance_matrix, -9999999, True, self.tour
    self.current_vertices[action] = 1
    done = len(self.tour) == 20
    if done:
      print(self.tour)
    return self.current_vertices, -self.distance_matrix, -distance, done, self.tour

  def close(self):
    return

class S2VNetwork(nn.Module):

  #num_vertices: The number of vertices in the graph.
  #num_levels: The number of hops in the graph to consider in the encoding
  #embedding_dim: The dimension of the final embedding
  def __init__(self, num_vertices, num_levels, embedding_dim):
    nn.Module.__init__(self)
    self.num_levels = num_levels
    self.num_vertices = num_vertices
    self.embedding_dim = embedding_dim
    self.theta1 = nn.Linear(2, self.embedding_dim)
    self.theta2 = nn.Linear(self.embedding_dim, self.embedding_dim)
    self.theta3 = nn.Linear(self.embedding_dim, self.embedding_dim)
    self.theta4 = nn.Linear(1, self.embedding_dim)

    torch.nn.init.xavier_uniform_(self.theta1.weight, 0.03)
    torch.nn.init.xavier_uniform_(self.theta2.weight, 0.03)
    torch.nn.init.xavier_uniform_(self.theta3.weight, 0.03)
    torch.nn.init.eye_(self.theta4.weight)

  #x: One hot encoding of vertices that have been visited (n entries), and end points
  #W: Distance matrix (n x n)
  #x and W are going to be batched (so they will have dimension batch_size times real dimension)
  #pseudo_forward can't do batching
  def single_forward(self, x, W):
    all_neighbors = W.topk(10, largest=False, dim=1)[1]
    u = torch.zeros((self.num_vertices, self.embedding_dim))
    for i in range(self.num_levels):
      u1 = self.theta1(x.reshape([len(x), 2])).reshape([self.num_vertices, self.embedding_dim])
      u2s = []
      u3s = []
      for vertex in range(len(x)):
        neighbor_u = u[all_neighbors[vertex]]
        u2s.append(self.theta2(torch.sum(neighbor_u, dim = 0)))
        weights = W[vertex][all_neighbors[vertex]].reshape([1, 10, 1])
        u3s.append(self.theta3(torch.sum(F.relu(self.theta4(weights)), dim=1)))

      u2 = torch.cat(u2s).reshape([self.num_vertices, self.embedding_dim])
      u3 = torch.cat(u3s).reshape([self.num_vertices, self.embedding_dim])

      u = (u1 + u2 + u3).reshape([self.num_vertices, self.embedding_dim])
    return u.reshape(self.num_vertices * self.embedding_dim)

  def forward(self, x, W):
    us = []
    for batch_elem in range(x.size(0)):
      us.append(self.single_forward(x[batch_elem], W[batch_elem]))
    return torch.cat(us, dim=0).reshape([x.size(0), self.num_vertices * self.embedding_dim])

class S2VNetwork_old(nn.Module):
  def __init__(self, num_vertices, num_levels, embedding_dim):
    nn.Module.__init__(self)
    self.num_levels = num_levels
    self.num_vertices = num_vertices
    self.embedding_dim = embedding_dim
    self.theta1 = nn.Linear(2 * self.num_vertices, self.num_vertices * self.embedding_dim)
    self.theta2 = nn.Linear(self.embedding_dim, self.embedding_dim)
    self.theta3 = nn.Linear(self.embedding_dim, self.embedding_dim)
    self.theta4 = nn.Linear(1, self.embedding_dim)

    torch.nn.init.xavier_uniform_(self.theta1.weight)
    torch.nn.init.xavier_uniform_(self.theta2.weight)
    torch.nn.init.xavier_uniform_(self.theta3.weight)
    torch.nn.init.xavier_uniform_(self.theta4.weight)

  #x: One hot encoding of vertices that have been visited (n entries) and endpoints
  #W: Distance matrix (n x n)
  #x and W are going to be batched (so they will have dimension batch_size times real dimension)
  def forward(self, x, W):
    us = []
    u = torch.zeros(x.size(0), self.num_vertices * self.embedding_dim)
    us.append(u)
    W_thing = W.reshape([x.size(0), self.num_vertices, self.num_vertices, 1])
    for i in range(self.num_levels):
      u = us[-1]
      u1 = self.theta1(x.flatten())
      u2 = torch.reshape(self.theta2(torch.sum(torch.reshape(u, [x.size(0), self.num_vertices, self.embedding_dim]), dim=1)).repeat(1, self.num_vertices), [x.size(0), self.num_vertices * self.embedding_dim])
      u3 = self.theta3(torch.sum(F.relu(self.theta4(W_thing)),
                                 dim = 1)).reshape([x.size(0),
                                                    self.num_vertices * self.embedding_dim])
      us.append(u1 + u2 + u3)
    return us[-1]

class QhatNetwork(nn.Module):
  def __init__(self, num_vertices):
    nn.Module.__init__(self)
    self.num_vertices = num_vertices
    self.embedding_dim = 64
    self.s2v = S2VNetwork(num_vertices, 4, self.embedding_dim)
    self.theta5 = nn.Linear(2 * self.embedding_dim * self.num_vertices, self.num_vertices)
    self.theta6 = nn.Linear(self.embedding_dim, self.embedding_dim * self.num_vertices)
    self.theta7 = nn.Linear(self.embedding_dim * self.num_vertices,
                            self.embedding_dim * self.num_vertices)

    torch.nn.init.xavier_uniform_(self.theta5.weight)
    torch.nn.init.xavier_uniform_(self.theta6.weight)
    torch.nn.init.xavier_uniform_(self.theta7.weight)

  def forward(self, x, W):
    u = self.s2v(x, W)
    x1 = self.theta7(u)
    x2 = self.theta6(torch.sum(torch.reshape(u, [x.size(0), self.num_vertices, self.embedding_dim]), dim=1))
    x = self.theta5(F.relu(torch.cat((x1, x2), 1)))
    return x

model = QhatNetwork(20)
target = QhatNetwork(20)
if use_cuda:
  model.cuda()
  target.cuda()
#optimizer = optim.Adam(model.parameters(), lr)
optimizer = optim.SGD(model.parameters(), lr, momentum=0.9)
steps_done = 0
episode_costs = []
test_costs = []
env = TSPEnv()

def select_epsilon_greedy_action(x, W):
  global steps_done
  sample = random.random()
  eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)
  print("Current Epsilon Threshold: {0}".format(eps_threshold))
  steps_done += 1
  next_vertex = 0
  if sample > eps_threshold:
    with torch.no_grad():
      mask = x[0, :, 0] < 1
      data = target(x, W).data[0]
      next_vertex = torch.argmax(data[mask]).view(1, 1)
  else:
    next_vertex = random.randrange(len(x[0]) - torch.nonzero(x[0, :, 0]).size(0))
  view = 0
  for i in range(len(x[0])):
    if x[0][i][0] == 0:
      view += 1
    if view == next_vertex + 1:
      return LongTensor([[i]])

def select_random_action(x, W):
  next_vertex = random.randrange(len(x[0]) - torch.nonzero(x[0, :, 0]).size(0))
  view = 0
  for i in range(len(x[0])):
    if x[0][i][0] == 0:
      view += 1
    if view == next_vertex + 1:
      return LongTensor([[i]])

def run_random_thing(e, environment, num_iters):
  x, W = environment.reset()
  for i in range(num_iters):
    x, W = environment.reset_tour()
    while True:
      action = select_random_action(FloatTensor([x]), FloatTensor([W]))
      next_x, next_W, reward, done, _ = environment.step(int(action[0, 0]))
      memory.push((FloatTensor([x]),
                   FloatTensor([W]),
                   action,
                   FloatTensor([next_x]),
                   FloatTensor([next_W]),
                   FloatTensor([reward]),
                   FloatTensor([int(done)])))

      x = next_x
      W = next_W

      if done:
        break

def run_episode(e, environment, test=False):
  pretrain = False
  x, W = environment.reset(pretrain)
  cost = 0
  while True:
    action = select_epsilon_greedy_action(FloatTensor([x]), FloatTensor([W]))
    next_x, next_W, reward, done, _ = environment.step(int(action[0, 0]))
    memory.push((FloatTensor([x]),
                 FloatTensor([W]),
                 action,
                 FloatTensor([next_x]),
                 FloatTensor([next_W]),
                 FloatTensor([reward]),
                 FloatTensor([int(done)])))

    learn()

    x = next_x
    W = next_W

    cost-=reward

    if done:
      if(test):
        test_costs.append(cost)
      episode_costs.append(cost)
      if len(episode_costs) % 10 == 0:
        print("Episode {0} finished with cost {1}".format(e, cost))
      break

def max_next_q_values(batch_next_x, batch_next_W):
  return target(batch_next_x, batch_next_W).detach().max(1)[0]

def learn(batch_size = 64):
  if(len(memory) < batch_size):
    return

  transitions = memory.sample(batch_size)
  batch_x, batch_W, batch_action, batch_next_x, batch_next_W, batch_reward, batch_done = zip(*transitions)
  batch_x = Variable(torch.reshape(torch.cat(batch_x), [batch_size, 20, 2]))
  batch_W = Variable(torch.reshape(torch.cat(batch_W), [batch_size, 20, 20]))
  batch_action = Variable(torch.cat(batch_action))
  batch_reward = Variable(torch.cat(batch_reward))
  batch_next_x = Variable(torch.cat(batch_next_x))
  batch_next_W = Variable(torch.cat(batch_next_W))
  batch_done = Variable(torch.cat(batch_done))

  current_q_values = model(batch_x, batch_W).gather(1, batch_action).squeeze()

  expected_future_rewards = max_next_q_values(batch_next_x, batch_next_W)
  expected_q_values = batch_reward + (gamma * expected_future_rewards) * (1 - batch_done)

  loss = F.mse_loss(current_q_values, expected_q_values)

  optimizer.zero_grad()
  loss.backward()
  optimizer.step()


if __name__ == "__main__":

  for e in range(11):
    print("Graph Number {0}".format(e))
    run_random_thing(e, env, 100)

  for i in range(5):
      learn(2000)

  target.load_state_dict(model.state_dict())

  print("Done Random Training")

  for e in range(num_episodes):
    t0 = time.time()
    run_episode(e, env)
    t1 = time.time()
    if e % 100 == 0:
      target.load_state_dict(model.state_dict())
    print("Episode {0} finished in {1} seconds".format(e, t1 - t0))
  print("Complete")
  env.close()

  test_env = TestEnv("/home/cbostanc/GitProjects/graph_comb_opt/data/tsp2d/test_tsp2d/tsp_min-n=15_max-n=20_num-graph=1000_type=clustered")
  for e in range(1000):
    run_episode(e, test_env, True)

  print(test_costs)

  plt.plot(test_costs)
  plt.show()

  test_env.close()
